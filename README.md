This application is for the gathering, storage, and display of Reddit data. 
I love large structures - behemoth applications which include a lot of moving parts. 

This is one of them.

# The Network
![This](ndexr.jpeg)
All incoming ports are blocked to external users except for 80 and 3000, the remaining ports are only accessible
 to approved IP addresses.

## My Programming History

I worked at a company called Digital First Media. I was hired on as a data optimization engineer. 
The job was primary working on their optimization code for online marketing campaigns. As the guy in-between, I worked
with qualified data engineers on one side of me and creative web developers on the other side.
 
[Duffy](https://github.com/duffn) was definitely one of the talented ones and taught me quite a bit. While I was with 
the company, one of my complaints was related to how much we were spending for tools we could easily make in house. Of 
of those choices, was whether to buy RSConnect or not. I found a way to build highly scalable R APIs using 
docker-compose and NGINX. Duffy was the guy who knew what was needed for a solution, so he gave me quite a bit of 
guidance in understanding good infrastructure. 

So that's where I learned about building really cool APIs so that I could share my output with non-R users. People used 
my APIs, Duffy was doing cool stuff in Data Engineering and was getting the data to me. 

I gravitated a bit out of the math into the tools Data Engineers used, and became interested in Python, 
SQL, Airflow etc. These guys spin up that stuff daily, so it's not impossible to learn! I started creating data 
pipelines, which grew - and became difficult to maintain. I wanted to learn best practices in data engineering - because 
when things break, it's devastating and a time sink and kept me up nights.  

AIRFLOW, is one of the tools for this job. It makes your scheduled jobs smooth like butter, and is highly transparent 
with the health of your network, and allows for push button runs of your code. This was far superior to cron jobs 
kicking off singular scripts.

                                                                                                                          
                                                                                                                          
## About This Project

The main components are 

1. An Airflow instance running scripts for data gathering.
2. A Postgres database to store the data, with scheduled backups to AWS S3.
3. An R Package I wrote for talking to AWS called `biggr` (using a Python backend - its an R wrapper for
 `boto3` using Reticulate)
4. An R Package I wrote for talking to Reddit called `redditor`  (using a Python backend - its an R wrapper for `praw` using Reticulate)  
5. An R API that converts the data generated by this pipeline to a front end application for display
6. A React Application which takes the data in the R API and displays it on the web.

## You will Need
1. Reddit API authentication
2. AWS IAM Creds (Not always)
3. Motivation to learn Docker, PostgreSQL, MongoDB, Airflow, R Packages using Reticulate, NGINX, 
 and web design.
4. Patience while Docker builds
5. An interest in programming
6. A pulse
7. Oxygen
8. Oreos


## Getting Started 
1. Request permission from `fdrennan` in NDEXR Slack for RStudio and Postgres access 
2. `FORK` this repository to your Github account
3. Run `git clone https://github.com/YOUR_GITHUB_USERNAME/ndexr-platform.git`
4. RUN `cd ndexr-platform`
5. RUN `docker build -t redditorapi --file ./DockerfileApi .`
6. RUN `docker build -t rpy --file ./DockerfileRpy .`
7. RUN `docker build -t redditorapp --file ./DockerfileShiny .`

#### Once these steps are complete, contact me to see how to set your environment variables.

## About the Dockerfiles
There are three dockerfiles that are needed: `DockerfileApi`, `DockerfileRpy`, and `DockerfileUi`

`DockerfileApi` is associated with the container needed to run an R [Plumber](https://www.rplumber.io/) API. 
In the container I take from [trestletech](https://hub.docker.com/r/trestletech/plumber/), I add on some additional 
Linux binaries and R packages. There are two R packages in this project. One is called [biggr] and the other is called 
[redditor], which are located in `./bigger` and `./redditor-api` respectively. To build the container, run the 
following:

```
docker build -t redditorapi --file ./DockerfileApi .
```

`DockerfileRpy` is a container running both R and Python, This is taken from the `python:3.7.6` container. I install R 
on top of it, so I can run scheduled jobs. This container runs Airflow, which is set up in `airflower`. Original name, 
right? 

```
docker build -t rpy --file ./DockerfileRpy .
```

This container contains code and packags required to run the Shiny application

```
docker build -t redditorapp --file ./DockerfileShiny .
```

# DAGz and Airflow

This is where all our Airflow dags live. R files are executed with a Bash Executor using the `Rscript` command. If you 
look in `./airflower/scripts/R/shell` you will see where the bash commands live. Let's take a look at one. This 
command `cd`'s into the `r_files` foler and runs the `streamall.R` R script. 


The command:
`. ./airflower/scripts/R/shell/streamall`

executes the following. 
```                                                                                                       
#!/bin/bash

cd /home/scripts/R/r_files
/usr/bin/Rscript /home/scripts/R/r_files/streamall.R
```

You can see the mapping of files in the project to the containers by the following - the left side is the file location
in the project directory and the ride side is in the container. 

`- ./airflower/scripts/R/shell/streamall:/home/scripts/R/shell/streamall`

Anyways, this kicks off the file here at `/home/scripts/R/r_files/streamall.R` which begins to grab Reddit data 
continuously. We see more environment variables we need to have. If you haven't already, go and get 
[Reddit API credentials](`https://www.reddit.com/wiki/api`).


This one manages the state of our dags.
```
  scheduler:
    image: rpy
    restart: always
    depends_on:
      - webserver
    env_file: .env
    environment:
      AIRFLOW_HOME: /root/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@host.docker.internal:5439/airflow
    volumes:
      - ./airflower/dags:/root/airflow/dags
      - ./airflower/scripts/sql:/home/scripts/sql
      - ./airflower/scripts/R/r_files/aws_configure.R:/home/scripts/R/r_files/aws_configure.R
      - ./airflower/scripts/R/r_files/r_venv_install.R:/home/scripts/R/r_files/r_venv_install.R
      - ./airflower/scripts/R/r_files/refresh_mat_comments_by_second.R:/home/scripts/R/r_files/refresh_mat_comments_by_second.R
      - ./airflower/scripts/R/r_files/refresh_mat_stream_authors.R:/home/scripts/R/r_files/refresh_mat_stream_authors.R
      - ./airflower/scripts/R/r_files/refresh_mat_submissions_by_second.R:/home/scripts/R/r_files/refresh_mat_submissions_by_second.R
      - ./airflower/scripts/R/r_files/stream_submissions_to_s3.R:/home/scripts/R/r_files/stream_submissions_to_s3.R
      - ./airflower/scripts/R/r_files/streamall.R:/home/scripts/R/r_files/streamall.R
      - ./airflower/scripts/R/r_files/streamsubmissions.R:/home/scripts/R/r_files/streamsubmissions.R
      - ./airflower/scripts/R/r_files/streamtos3.R:/home/scripts/R/r_files/streamtos3.R
      - ./airflower/scripts/R/shell/aws_configure:/home/scripts/R/shell/aws_configure
      - ./airflower/scripts/R/shell/refresh_mat_comments_by_second:/home/scripts/R/shell/refresh_mat_comments_by_second
      - ./airflower/scripts/R/shell/refresh_mat_submissions_by_second:/home/scripts/R/shell/refresh_mat_submissions_by_second
      - ./airflower/scripts/R/shell/stream_submissions_all:/home/scripts/R/shell/stream_submissions_all
      - ./airflower/scripts/R/shell/streamtos3:/home/scripts/R/shell/streamtos3
      - ./airflower/scripts/R/shell/r_venv_install:/home/scripts/R/shell/r_venv_install
      - ./airflower/scripts/R/shell/refresh_mat_stream_authors:/home/scripts/R/shell/refresh_mat_stream_authors
      - ./airflower/scripts/R/shell/stream_submission_to_s3:/home/scripts/R/shell/stream_submission_to_s3
      - ./airflower/scripts/R/shell/streamall:/home/scripts/R/shell/streamall
      - ./airflower/plugins:/root/airflow/plugins
      - ./.env:/home/scripts/R/r_files/.Renviron
      - airflow-worker-logs:/root/airflow/logs
    links:
      - "postgres"
    command: airflow scheduler
```

Two R APIS that are exactly the same except for the port location. These are load balanced by the `web` container and the `nginx.conf` file.

Again the `- filelocationlocal:filelocationcontainer` syntax explains how this project is connected.

```
  redditapi:
    image: redditorapi
    command: /app/plumber.R
    restart: always
    ports:
     - "8000:8000"
    working_dir: /app
    volumes:
      - ./plumber.R:/app/plumber.R
      - ./.env:/app/.Renviron
    links:
      - "postgres"
  redditapitwo:
    image: redditorapi
    command: /app/plumber.R
    restart: always
    ports:
     - "8001:8000"
    working_dir: /app
    volumes:
      - ./plumber.R:/app/plumber.R
      - ./.env:/app/.Renviron
    links:
      - "postgres"
```


# Useful quick commands

```
docker exec -it  redditor_scheduler_1  /bin/bash
docker exec -it  redditor_postgres_1  /bin/bash
docker exec -it  redditor_userinterface_1  /bin/bash
```

# Backing Up Your Data
```
psql -U airflow postgres < postgres.bak
```

```
scp -i "~/ndexr.pem" ubuntu@ndexr.com:/var/lib/postgresql/postgres/backups/postgres.bak postgres.bak
docker exec redditor_postgres_1 pg_restore -U airflow -d postgres /postgres.bak
```


# Dont run these unless you know what you are doing. Im serious.
```
docker stop $(docker ps -a -q)
docker rm $(docker ps -a -q)
docker volume prune
docker volume rm  redditor_postgres_data
```



docker exec -it   redditor_scheduler_1  /bin/bash
docker exec -it   redditor_backup_1  /bin/bash
docker exec -it   redditor_redditapi_1  /bin/bash
docker exec -it   redditor_postgres  /bin/bash


pg_dump -h db -p 5432 -Fc -o -U postgres postgres > postgres.bak
wget https://redditor-dumps.s3.us-east-2.amazonaws.com/postgres.tar.gz
tar -xzvf postgres.tar.gz




docker exec -it   airflow_scheduler_1  /bin/bash
## Restore Database
Run Gathering Dag
```
docker exec -it   redditor_postgres  /bin/bash 
tar -zxvf /data/postgres.tar.gz
pg_restore --clean --verbose -U postgres -d postgres /postgres.bak
# /var/lib/postgresql/data
```
 

# To Kill a port
sudo fuser -k -n tcp 3000


# UPDATING/ALLLOWING PORTS
sudo systemctl restart ssh
sudo vim /etc/ssh/sshd_config
/var/log/secure
AllowTcpForwarding yes
GatewayPorts yes

### LENOVO
`autossh -f -nNT -i /home/fdrennan/ndexr.pem -R 2300:localhost:22 -R 8999:localhost:8999 -R 3000:localhost:3000 -R 8000:localhost:8000 -R 8001:localhost:8001 -R 8002:localhost:8002 -R 8003:localhost:8003 -R 8004:localhost:8004 ubuntu@ndexr.com -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ExitOnForwardFailure=yes`

### DELL XPS
`autossh -f -nNT -i /home/fdrennan/ndexr.pem -R 2500:localhost:22 -R 9200:localhost:9200 -R 8081:localhost:8081 ubuntu@ndexr.com -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ExitOnForwardFailure=yes`

### POWEREDGE
`autossh -f -nNT -i /home/fdrennan/ndexr.pem -R 2400:localhost:22 -R 8787:localhost:8787 -R 5433:localhost:5432 ubuntu@ndexr.com -o UserKnownHostsFile=/dev/null -o StrictHostKeyChecking=no -o ExitOnForwardFailure=yes`


# Uploading to Docker
```
docker image tag rpy:latest fdrennan/rpy:latest
docker push fdrennan/rpy:latest

docker image tag redditorapi:latest fdrennan/redditorapi:latest
docker push fdrennan/redditorapi:latest
```

# Check open ports
https://gf.dev/port-scanner

# Specify Docker Compose Location
docker-compose -f /Users/fdrennan/redditor/do.yml up
https://analytics.google.com/analytics/web/#/

# Reset Life

```
docker stop $(docker ps -a -q)
docker rm $(docker ps -a -f status=exited -q)
docker rmi $(docker images -a -q)
docker volume prune
```

# Install Elastic Search Plugins
https://serverfault.com/questions/973325/how-to-install-elasticsearch-plugins-with-docker-container

# Add user
`sudo adduser newuser`

`usermod -aG sudo newuser`

# Monitoring Users
https://www.ostechnix.com/monitor-user-activity-linux/